inferenceService:
  name: cpu-inference-service
  minReplicas: 1
  maxReplicas: 1
  resources:
    requests:
      cpu: "2"
      memory: 4Gi
    limits:
      cpu: "4"
      memory: 8Gi
  affinity: {}
  tolerations: {}

servingRuntime:
  name: cpu-runtime
  port: 8080
  image: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.10.2
  modelFormat: huggingface
  args:
    - --model
    - ibm-granite/granite-4.0-350m

dsc:
  initialize: true
  kserve:
    defaultDeploymentMode: RawDeployment
    rawDeploymentServiceConfig: Headed

externalSecret:
  create: true
